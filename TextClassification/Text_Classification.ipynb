{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62ce093d-413d-4264-a0cb-3f64cf3167d3",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "517f5bb9-6486-46d0-9c8f-f8fcbfff6c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, Doc2Vec, doc2vec, KeyedVectors\n",
    "from gensim.utils import simple_preprocess, tokenize\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7176c77-684b-49fc-8d15-aac5dd08a540",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load news categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a05f7ae-3a6c-4224-a8ab-cbed3c942397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'rec.motorcycles',\n",
    "    'sci.med',\n",
    "    'sci.space',\n",
    "    'talk.politics.guns'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b01a98-3125-4d98-9226-e3c2d52fc4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['rec.motorcycles', 'sci.med', 'sci.space', 'talk.politics.guns']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88291b2f-3f25-4c78-b1f8-44eef73ac118",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Fetch documents for listed categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb4ce024-ab76-48e5-8879-e1a5ab39c629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2331 train documents\n",
      "1552 test documents\n",
      "4 categories\n"
     ]
    }
   ],
   "source": [
    "train_data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories)\n",
    "print(f\"{len(train_data.filenames)} train documents\")\n",
    "print(f\"{len(test_data.filenames)} test documents\")\n",
    "print(f\"{len(train_data.target_names)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9da1ae8-0dd7-4b89-9835-4556661f8ddc",
   "metadata": {},
   "source": [
    "**For text feature extractor, we will be using 2 of them.**\n",
    "\n",
    "1. HashingVectorizer\n",
    "2. TfidfVectorizer (same as CountVectorizer + TfidfTransformer)\n",
    "3. Word2Vec\n",
    "4. Doc2Vec\n",
    "\n",
    "**For Classifier, we will be using 4 of them**\n",
    "\n",
    "1. Multinomial Naive Bayes\n",
    "2. Logistic Regression\n",
    "3. Support Vector Machine\n",
    "4. Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7785f3ac-18e4-4510-a8eb-e9b2dc77d61a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Vectorizing available data for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9fd5ff10-0132-4029-bbbe-d933fb70195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = \"GoogleNews-vectors-negative300.bin\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
    "text_processed = [simple_preprocess(sent) for sent in train_data.data]\n",
    "\n",
    "def embedding_features(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this = np.zeros(DIMENSION)\n",
    "        count_for_this = 0\n",
    "        for token in tokens:\n",
    "            if token in w2v_model:\n",
    "                feat_for_this += w2v_model[token]\n",
    "                count_for_this += 1\n",
    "        feats.append(feat_for_this/count_for_this)\n",
    "    return feats\n",
    "\n",
    "w2v_train_vectors = embedding_features(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2aa47-d3f8-43b3-a560-f283b3c13994",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Vectorizing available data for Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd4f139b-f12d-4af9-aa6e-f0d7e7dd65d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_train = [doc2vec.TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(text_processed)]\n",
    "\n",
    "model = Doc2Vec(size=300, alpha=0.025, epochs=100)\n",
    "model.build_vocab(d2v_train)\n",
    "model.train(d2v_train, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "vocab = model.wv.vocab\n",
    "\n",
    "def embedding_features(list_of_lists):\n",
    "    DIMENSION = 300\n",
    "    zero_vector = np.zeros(DIMENSION)\n",
    "    feats = []\n",
    "    for tokens in list_of_lists:\n",
    "        feat_for_this = np.zeros(DIMENSION)\n",
    "        count_for_this = 0\n",
    "        for token in tokens:\n",
    "            if token in vocab.keys():\n",
    "                feat_for_this += model[token]\n",
    "                count_for_this += 1\n",
    "        feats.append(feat_for_this / count_for_this)\n",
    "    return feats\n",
    "\n",
    "d2v_train_vectors = embedding_features(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c296849d-5248-4194-9209-eb8e34f4121f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using MultinomialNB with all 4 Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a9078-6b1f-4139-890c-d7e9c5e417fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HashingVectorizer with MultinomialNavieBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95b3169c-ba12-4827-b78b-631b81e215ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best score: 0.965\n",
      "Best parameters set:\n",
      "\thash__ngram_range: (1, 2)\n",
      "\thash__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_hash_mnb = Pipeline([\n",
    "    (\"hash\", HashingVectorizer(alternate_sign=False)),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'hash__norm': ('l1', 'l2'),\n",
    "    'hash__ngram_range': ((1, 1), (1, 2), (1, 3)),  # unigrams or bigrams\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_hash_mnb, pipe_parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5e22c-d299-43e2-98e9-af722c85c572",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TfidfVectorizer with MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e5af92f3-3f77-4326-8ff7-b9aa7a624347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best score: 0.976\n",
      "Best parameters set:\n",
      "\ttfidf__min_df: 0\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_tfidf_mnb = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2), (1, 3)),  # unigrams or bigrams\n",
    "    'tfidf__min_df': (0, 0.1, 0.25, 0.5),\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_tfidf_mnb, pipe_parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c21d54-c07d-4f34-aef4-e8ef8fe78b5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vec with MultinomialNB - Can't train because of negative values in training vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c689e0bb-0f71-44a8-9519-e84045621d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best score: 0.846\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_w2v_mnb = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__alpha': (0.25, 0.50, 0.75, 1),\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_w2v_mnb, pipe_parameters, cv=5, n_jobs=12, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(w2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca63d09-6688-45c0-a068-1f39aba2adf3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Doc2Vec with MultinomialNB - Can't train because of negative values in training vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bdbb9d6b-b607-45fc-9da1-dfa7c202f89f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best score: 0.927\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_d2v_mnb = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__alpha': (0.25, 0.50, 0.75, 1),\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_d2v_mnb, pipe_parameters, cv=5, n_jobs=12, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(d2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12836c09-69e3-4d4a-bcf9-37fc2e2d0719",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Logistic Regression with all 4 Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8daa4-dcae-4e67-8d6e-90e82fb6a46c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HashingVectorizer with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ac44b54-c731-4ead-86be-8578d1e200f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prite\\miniconda3\\envs\\ml-venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.974\n",
      "Best parameters set:\n",
      "\tclf__C: 0.0001\n",
      "\tclf__penalty: 'none'\n",
      "\tclf__solver: 'newton-cg'\n",
      "\thash__ngram_range: (1, 2)\n",
      "\thash__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_hash_lr = Pipeline([\n",
    "    (\"hash\", HashingVectorizer()),\n",
    "    (\"clf\", LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'hash__norm': ('l1', 'l2'),\n",
    "    'hash__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams,\n",
    "    'clf__penalty': ('l2', 'none'),\n",
    "    'clf__solver': ('newton-cg', 'lbfgs'),\n",
    "    'clf__C': (0.0001, 0.001, 0.01)\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_hash_lr, pipe_parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf47533-ed9c-47be-a9f5-7f0889ee3f2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TfidfVectorizer with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5489438-f0e1-4dc0-a427-cb4f2a4c9a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 128 candidates, totalling 256 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prite\\miniconda3\\envs\\ml-venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.976\n",
      "Best parameters set:\n",
      "\tclf__C: 0.0001\n",
      "\tclf__penalty: 'none'\n",
      "\tclf__solver: 'newton-cg'\n",
      "\ttfidf__min_df: 0\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__norm: 'l1'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_tfidf_lr = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__min_df': (0, 0.1, 0.25, 0.5),\n",
    "    'clf__penalty': ('l2', 'none'),\n",
    "    'clf__solver': ('newton-cg', 'lbfgs'),\n",
    "    'clf__C': (0.0001, 0.001)\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_tfidf_lr, pipe_parameters, cv=2, n_jobs=12, verbose=3)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf411c-f6a5-46c6-8e18-b870fcbb3434",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vec with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "595e6dc7-590e-41c9-aa9e-c7c543cac669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prite\\miniconda3\\envs\\ml-venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.957\n",
      "Best parameters set:\n",
      "\tclf__C: 0.01\n",
      "\tclf__penalty: 'none'\n",
      "\tclf__solver: 'sag'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prite\\miniconda3\\envs\\ml-venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_w2v_lr = Pipeline([\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__penalty': ('l2', 'none'),\n",
    "    'clf__solver': ('newton-cg', 'lbfgs', 'sag', 'saga'),\n",
    "    'clf__C': (0.0001, 0.001, 0.01, 1)\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_w2v_lr, pipe_parameters, cv=5, n_jobs=-1, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(w2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065b2ec-18d4-4f29-8420-b605532011b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Doc2Vec with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9d4d938a-b457-46db-aeee-b50b2fd8ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best score: 0.961\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__penalty: 'l2'\n",
      "\tclf__solver: 'newton-cg'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_d2v_lr = Pipeline([\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__penalty': ('l2', 'none'),\n",
    "    'clf__solver': ('newton-cg', 'lbfgs', 'sag', 'saga'),\n",
    "    'clf__C': (0.0001, 0.001, 0.01, 1)\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_d2v_lr, pipe_parameters, cv=5, n_jobs=-1, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(d2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d4347-7ccd-4284-8f91-1d105186cc38",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Support Vector Machines with all 4 Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1a7717-e5d0-4ab3-bb62-0b012cb286b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HashingVectorizer with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e901a1-aaa4-43a1-ab2b-b0ff2c7c1647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "Best score: 0.819\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__gamma: 'scale'\n",
      "\tclf__kernel: 'linear'\n",
      "\thash__ngram_range: (1, 2)\n",
      "\thash__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_hash_svm = Pipeline([\n",
    "    (\"hash\", HashingVectorizer()),\n",
    "    (\"clf\", SVR())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'hash__norm': ('l1', 'l2'),\n",
    "    'hash__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams,\n",
    "    'clf__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'clf__gamma': ('scale', 'auto'),\n",
    "    'clf__C': (0.001, 0.01, 1),\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_hash_svm, pipe_parameters, cv=3, n_jobs=12, verbose=3)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75496e6c-48c5-4095-8557-8857bfd69a6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TfidfVectorizer with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "945182c8-b7db-4892-a705-82a6532e15a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 384 candidates, totalling 768 fits\n",
      "Best score: 0.842\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__gamma: 'scale'\n",
      "\tclf__kernel: 'sigmoid'\n",
      "\ttfidf__min_df: 0\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_tfidf_svm = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", SVR())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__min_df': (0, 0.1, 0.25, 0.5),\n",
    "    'clf__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'clf__gamma': ('scale', 'auto'),\n",
    "    'clf__C': (0.001, 0.01, 1),\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_tfidf_svm, pipe_parameters, cv=2, n_jobs=12, verbose=3)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750beb52-8b40-4b96-8df5-a5915ebc6bae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vecth SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2545006b-6a42-4060-8150-78821bcf75c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best score: 0.764\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__gamma: 'scale'\n",
      "\tclf__kernel: 'poly'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_w2v_svm = Pipeline([\n",
    "    (\"clf\", SVR())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'clf__gamma': ('scale', 'auto'),\n",
    "    'clf__C': (0.001, 0.01, 1),\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_w2v_svm, pipe_parameters, cv=5, n_jobs=-1, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(w2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b47c0-7ab2-48e5-97ee-9c05866eeaa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Doc2Vec with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3d576779-0f45-480e-b6ff-43fbd8081f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best score: 0.830\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__gamma: 'scale'\n",
      "\tclf__kernel: 'rbf'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_d2v_svm = Pipeline([\n",
    "    (\"clf\", SVR())\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'clf__gamma': ('scale', 'auto'),\n",
    "    'clf__C': (0.001, 0.01, 1),\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_d2v_svm, pipe_parameters, cv=5, n_jobs=-1, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(d2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84093887-0416-4267-a0e8-1ac37418d3d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using Decision Trees with all 4 vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc39a2-ce68-4db0-9b3a-5da59f8f4665",
   "metadata": {
    "tags": []
   },
   "source": [
    "### HashingVectorizer with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ec425a-8fd2-446d-9911-da9434dd9449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "Best score: 0.650\n",
      "Best parameters set:\n",
      "\tclf__criterion: 'gini'\n",
      "\tclf__max_depth: 100\n",
      "\tclf__max_features: 'sqrt'\n",
      "\tclf__splitter: 'best'\n",
      "\thash__ngram_range: (1, 2)\n",
      "\thash__norm: 'l2'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_hash_dt = Pipeline([\n",
    "    (\"hash\", HashingVectorizer()),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'hash__norm': ('l1', 'l2'),\n",
    "    'hash__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams,\n",
    "    'clf__criterion': ('gini', 'entropy', 'log_loss'),\n",
    "    'clf__splitter': ('best', 'random'),\n",
    "    'clf__max_depth': (25, 50, 100),\n",
    "    'clf__max_features': ('sqrt', 'log2')\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_hash_dt, pipe_parameters, cv=3, n_jobs=12, verbose=3)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a4a9c4-468a-4e6f-8298-91ae14c7c1f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TfidfVectorizer with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "190609c0-fa08-48d9-b863-2a45dacad851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 576 candidates, totalling 1728 fits\n",
      "Best score: 0.689\n",
      "Best parameters set:\n",
      "\tclf__criterion: 'entropy'\n",
      "\tclf__max_depth: 100\n",
      "\tclf__max_features: 'sqrt'\n",
      "\tclf__splitter: 'best'\n",
      "\ttfidf__min_df: 0\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__norm: 'l1'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_tfidf_dt = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__min_df': (0, 0.1, 0.25, 0.5),\n",
    "    'clf__criterion': ('gini', 'entropy', 'log_loss'),\n",
    "    'clf__splitter': ('best', 'random'),\n",
    "    'clf__max_depth': (25, 50, 100),\n",
    "    'clf__max_features': ('sqrt', 'log2')\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_tfidf_dt, pipe_parameters, cv=3, n_jobs=12, verbose=3)\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(train_data.data, train_data.target)\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40356194-8107-4af4-bed1-cd9344f7e928",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Word2Vec with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72a72a0f-11cc-4a30-a258-86a14a13e60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best score: 0.633\n",
      "Best parameters set:\n",
      "\tclf__criterion: 'gini'\n",
      "\tclf__max_depth: 25\n",
      "\tclf__max_features: 'sqrt'\n",
      "\tclf__splitter: 'best'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_w2v_dt = Pipeline([\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__criterion': ('gini', 'entropy', 'log_loss'),\n",
    "    'clf__splitter': ('best', 'random'),\n",
    "    'clf__max_depth': (25, 50, 100),\n",
    "    'clf__max_features': ('sqrt', 'log2')\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_w2v_dt, pipe_parameters, cv=5, n_jobs=-1, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(w2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d389d-14ea-48f5-9c5c-51b425342710",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Doc2Vec with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a9e87f8-6790-4ce2-ac6c-a4303bc106b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best score: 0.618\n",
      "Best parameters set:\n",
      "\tclf__criterion: 'entropy'\n",
      "\tclf__max_depth: 25\n",
      "\tclf__max_features: 'sqrt'\n",
      "\tclf__splitter: 'best'\n"
     ]
    }
   ],
   "source": [
    "# Creating pipleine\n",
    "pipeline_d2v_dt = Pipeline([\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Defining pipeline parameters\n",
    "pipe_parameters = {\n",
    "    'clf__criterion': ('gini', 'entropy', 'log_loss'),\n",
    "    'clf__splitter': ('best', 'random'),\n",
    "    'clf__max_depth': (25, 50, 100),\n",
    "    'clf__max_features': ('sqrt', 'log2')\n",
    "}\n",
    "\n",
    "# Creating gridsearch instance\n",
    "grid_search = GridSearchCV(pipeline_d2v_dt, pipe_parameters, cv=5, n_jobs=-1, verbose=1, error_score=\"raise\")\n",
    "\n",
    "# Fitting data into GridSearch Instance\n",
    "grid_search.fit(d2v_train_vectors, train_data.target)\n",
    "\n",
    "\n",
    "# Getting best score from instance\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "\n",
    "# Getting best parameters for classifiers and vectorizers\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(pipe_parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f4442-85c6-4fbe-aff6-0425a49c6231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "ml-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
